{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from ipywidgets import interact, interactive, fixed, IntSlider, Dropdown\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "from fastai.data.all import *\n",
    "from fastai.tabular.all import *\n",
    "from fastai.torch_core import set_seed\n",
    "\n",
    "from scheeg import data\n",
    "from scheeg.performance import accuracy, model_accuracy, sensitivity_specificity_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A snippet of code from [fastbook](https://github.com/fastai/fastbook) to draw a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs):\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n",
    "                      special_characters=True, rotate=False, precision=precision, **kwargs)\n",
    "    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some settings for *Pandas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some colors to be used in LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_colors = {\n",
    "    'red',\n",
    "    'Bittersweet',\n",
    "    'blue',\n",
    "    'Cerulean',\n",
    "    'Violet',\n",
    "    'Goldenrod',\n",
    "    'ForestGreen',\n",
    "#     'Gray',\n",
    "    'YellowOrange',\n",
    "    'RubineRed',\n",
    "    'RoyalBlue',\n",
    "    'Fuchsia',\n",
    "    'OliveGreen',\n",
    "    'Black'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n",
    "\n",
    "> Experiments on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexes of the subjects that are reserved for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_testing_subjects = [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those that, during training, will only be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_validation_subjects = [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable is named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'ill'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of *folds* when performing cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds_cross_validation = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path where results are to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = pathlib.Path.home() / 'papers/frontiers_2020/results3'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the EEG channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_channels_names = [\n",
    "    'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'C3', 'Cz', 'C4',\n",
    "    'P3', 'Pz', 'P4', 'T3', 'T4', 'T5', 'T6', 'O1', 'O2']\n",
    "assert len(eeg_channels_names) == 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mapping from indexes to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_index_to_name = dict(list(enumerate(eeg_channels_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the MATLAB data file is assembled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = pathlib.Path.cwd() / 'preprocessed_data'\n",
    "input_file = input_dir / 'CON_MAT.mat'\n",
    "assert input_file.exists()\n",
    "input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MATLAB* file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, (n_subjects, n_channels, _, n_frequency_bands, n_samples) = data.read_matlab(input_file)\n",
    "print(f'{n_subjects=}, {n_channels=}, {n_frequency_bands=}, {n_samples=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['GPDC_H'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Raw* frequency bands are combined into EEG bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_band_from_raw_freq = {\n",
    "    'delta': range(4),\n",
    "    'theta': range(4,8),\n",
    "    'alpha': range(8,12),\n",
    "    'beta': range(12,30),\n",
    "    'gamma': range(30,50)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience function to write a connectivity matrix in a format suitable to be plot by *pgfplots* as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectivity_matrix_to_pgf(matrix: np.ndarray, output_file: Union[str, pathlib.Path], meta:str = '') -> None:\n",
    "\n",
    "    # header\n",
    "    with output_file.open('w') as f:\n",
    "        f.write(f'# {meta}\\n')\n",
    "        f.write(f'# <source channel> <destination channel> <metric>\\n')\n",
    "\n",
    "    # data\n",
    "    with output_file.open('a') as f:\n",
    "        for i_coord, coord in enumerate(itertools.product(np.arange(n_channels), repeat=2), start=1):\n",
    "            f.write(f'{coord[1]} {coord[0]} {matrix[coord[1],coord[0]]}\\n')\n",
    "\n",
    "            # a newline every time a column is processed\n",
    "            if i_coord % n_channels == 0:\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write files for ill and healthy subjects using the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, meta, output in zip(['GPDC_S'], ['GPDC, alpha band, ill subject #0, sample #0'], ['connectivity_ill.txt']):\n",
    "for k, meta, output in zip(\n",
    "    ['GPDC_S', 'GPDC_H'],\n",
    "    ['GPDC, alpha band, ill subject #0, sample #0', 'GPDC, alpha band, healthy subject #0, sample #0'],\n",
    "    ['connectivity_ill.txt', 'connectivity_healthy.txt']):\n",
    "    averaged_freqs = data.average_frequencies(metrics[k], eeg_band_from_raw_freq.values())\n",
    "    connectivity_matrix_example = averaged_freqs[0, ..., 2, 0]\n",
    "    connectivity_matrix_to_pgf(connectivity_matrix_example, output_path / output, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After averaging over frequencies, these are the dimensions of the resulting arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_freqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, the dimensions of a *single* connectiviy matrix are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_matrix_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to build a `DataFrame` from *healthy* and *ill* samples for a given metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_to_df(healthy: np.array, ill: np.array, columns_name_prefix: str, dep_var: str = 'ill') -> pd.DataFrame:\n",
    "    \n",
    "    pre_processed_data = [\n",
    "        data.to_df(\n",
    "            *data.images_to_vectors(\n",
    "                data.average_frequencies(array, eeg_band_from_raw_freq.values()),\n",
    "                both=True, name_prefix=columns_name_prefix\n",
    "            )\n",
    "        )\n",
    "        for array in [healthy, ill]\n",
    "    ]\n",
    "    \n",
    "    pre_processed_data[0][dep_var] = False\n",
    "    pre_processed_data[1][dep_var] = True\n",
    "    \n",
    "    return pd.concat(pre_processed_data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function is applied on *GPDC* metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPDC = metric_to_df(metrics['GPDC_H'], metrics['GPDC_S'], columns_name_prefix='GPDC_', dep_var=dep_var)\n",
    "df_GPDC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPDC.shape[1] - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjects are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPDC['subject'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every *subject* above actually **encompasses two different subjects** since we have `n_subjects` healthy subjects and `n_subjects` ill ones, and they are labeled the same. In other words, the $n$-th subject encompasses the samples of the $n$-th healthy subject and those of the $n$-th ill subject. Hence, the number of samples for the, e.g., $0$-th *subject* is twice the number of actual samples we have for any given subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_GPDC.groupby('subject').size().loc[0] == n_samples * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above remark entails that we have the same number of *healthy* and *ill* labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GPDC[dep_var].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dDTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dDTF = metric_to_df(metrics['dDTF_H'], metrics['dDTF_S'], columns_name_prefix='dDTF_', dep_var=dep_var)\n",
    "df_dDTF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAVEAT**: notice that every *subject* in the `DataFrame` actually encompasses data from two subjects, one healthy and one sick. Moreover, this (arbitrary) pairing of subjects affects data splitting below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ill` columns are the same in both `DataFrame`s\n",
    "assert df_GPDC['ill'].equals(df_dDTF['ill'])\n",
    "\n",
    "# idem for `subject`\n",
    "assert df_GPDC['subject'].equals(df_dDTF['subject'])\n",
    "\n",
    "# we get rid of those in, e.g., `df_GPDC`, before concatenating\n",
    "df = pd.concat((df_GPDC.drop(['ill', 'subject'], axis=1), df_dDTF), axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexes in the concatenated `DataFrame`s were the same, and so we have duplicated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[1] - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to split the data into training and validation according to the indexes of *paired* subjects. That is, subject #3 means healthy subject #3 and ill subject #3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_on_paired_subjects(df: pd.DataFrame, i_validation_subjects: list) -> Tuple[list, list]:\n",
    "    \n",
    "    belong_in_validation = df['subject'].isin(i_validation_subjects)\n",
    "    i_validation = np.where(belong_in_validation)[0]\n",
    "    i_training = np.where(~belong_in_validation)[0]\n",
    "    \n",
    "    return sorted(i_training), sorted(i_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_training, i_validation = split_data_on_paired_subjects(df, [0])\n",
    "i_training[:4], i_validation[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples in the training and validation sets. The latter must be `n_samples` $\\times$ $2$ subjects (one healthy, and ill) $\\times$ `len(i_validation_subjects)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i_training), len(i_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to split the data considering separately healthy and ill subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_on_individual_subjects(\n",
    "    df: pd.DataFrame, i_healthy_subjects_in_valid: list, i_ill_subjects_in_valid: list) -> Tuple[list, list]:\n",
    "    \n",
    "    healthy_in_validation = (df['subject'].isin(i_healthy_subjects_in_valid)) & (~ df['ill'])\n",
    "    ill_in_validation = (df['subject'].isin(i_ill_subjects_in_valid)) & df['ill']\n",
    "    \n",
    "    i_validation = np.nonzero((healthy_in_validation | ill_in_validation).to_numpy())[0].tolist()\n",
    "    i_training = list(set(range(len(df))) - set(i_validation))\n",
    "    \n",
    "    assert len(df) == len(i_validation) + len(i_training)\n",
    "    \n",
    "    return sorted(i_training), sorted(i_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An arbitrary example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_training, i_validation = split_data_on_individual_subjects(df, [0, 1], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_training[:4], i_validation[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples in the training and validation sets. The latter must be `n_samples` $\\times$ (`len(i_healthy_subjects_in_valid)` + `len(i_ill_subjects_in_valid)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(i_training), len(i_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjects in `i_testing_subjects` are excluded from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df.loc[~df['subject'].isin(i_testing_subjects)]\n",
    "df_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of ill and healthy subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training[dep_var].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexes for the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_training, i_validation = split_data_on_paired_subjects(df_training, i_validation_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of samples in *training* and *validation* brings back the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(i_validation) + len(i_training)) == len(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the indexes are disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(i_validation).intersection(i_training) == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *data columns* (as opposed to the *outcome column* or the *subject column*) can be identified because they contain numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = df.filter(regex=r'\\d+').columns.to_list()\n",
    "data_columns[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fastai *tabular object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularPandas(df_training, cont_names=data_columns, y_names=dep_var, splits=(i_training, i_validation))\n",
    "type(to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of samples in the *training* and *validation* sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to.train), len(to.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, we set variables for easy access to the independent and dependent variables in both the *training* and *validation* sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, y = to.train.xs,to.train.y\n",
    "valid_xs, valid_y = to.valid.xs,to.valid.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_leaf_nodes=4)\n",
    "model.fit(xs, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tree is drawn. Notice that we are renaming the columns so that [graphviz](https://graphviz.org/) (on which `draw_tree` relies) doesn't get confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(\n",
    "    model, xs.rename(lambda x: x.replace('->', '').replace('_', ''), axis='columns'), leaves_parallel=True, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fit with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
    "model.fit(xs, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many leaves result? Is that a lot relative to the size of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_n_leaves(), len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model is **classifier** (rather than a regressor), and hence it predicts integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(valid_xs).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is possible to estimate the probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(valid_xs)[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding *decisions* are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(valid_xs)[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the classes (and hence the probability of a subject being *ill* is in the 2nd column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(model, xs, y), model_accuracy(model, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple *random forest regressor* with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(xs, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training* and *validation* accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(model, xs, y), model_accuracy(model, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience function to build and fit in one go a *random forest*. It returns the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(xs, y, n_estimators=200, max_samples=None,\n",
    "       max_features=0.025, min_samples_leaf=10):\n",
    "    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n",
    "        max_features=max_features, max_samples=max_samples,\n",
    "        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf(xs, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(model, xs, y), model_accuracy(model, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-bag (OOB) predictions are much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy(model.oob_prediction_,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for every sample (in the *validation* set) from each individual tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trees_predictions = np.stack([m.predict(valid_xs) for m in model.estimators_])\n",
    "all_trees_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above validation accuracy is simply the mean across all the tress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(all_trees_predictions.mean(0), valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation across tress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = all_trees_predictions.std(axis=0)\n",
    "sds.min(), sds.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction as we increase the number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = all_trees_predictions.shape[0]\n",
    "n_trees_accuracy = np.array(\n",
    "    [accuracy(all_trees_predictions[:i+1].mean(axis=0), valid_y) for i in range(n_trees)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, n_trees+1), n_trees_accuracy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the above process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience function to split the data into *training*, *validation* and *testing* datasets, and fit a random forest to the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_and_fit(df: pd.DataFrame, i_testing_subjects: list, i_validation_subjects: list):\n",
    "\n",
    "    # the names of the columns, which are NOT the dependent variable\n",
    "    data_columns = df.filter(regex=r'\\d+').columns.to_list()\n",
    "\n",
    "    # training data is filtered out of the full dataset\n",
    "    df_training = df.loc[~df['subject'].isin(i_testing_subjects)]\n",
    "    \n",
    "    i_training, i_validation = split_data_on_paired_subjects(df_training, i_validation_subjects)\n",
    "\n",
    "    # fastai data object\n",
    "    to = TabularPandas(df_training, cont_names=data_columns, y_names=dep_var, splits=(i_training, i_validation))\n",
    "\n",
    "    # the model is built and fitted\n",
    "    model = rf(to.train.xs, to.train.y)\n",
    "\n",
    "    return model, to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience function to combine the names and importance (according to the random forest) of the different features into a pandas `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_series(model, xs) -> pd.Series:\n",
    "    \n",
    "    return pd.Series(data=model.feature_importances_, index=xs.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = feature_importance_series(model, xs)\n",
    "fi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst features are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi.sort_values()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen there is a mixture of *GPDC* and *dTFT* features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{(fi > 0).sum()} features (out of {len(fi)}) contribute something')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set a threshold,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 5e-3\n",
    "threshold = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and keep only those features whose importance is above it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_above_threshold = fi > threshold\n",
    "feature_above_threshold.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = fi[feature_above_threshold]\n",
    "important_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_important = xs[important_features.index]\n",
    "xs_important.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf(xs_important, y);\n",
    "model_accuracy(model, xs_important, y), model_accuracy(model, valid_xs[important_features.index], valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, nothing really changed by dismissing quite a bunch of features, specifically,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape[1] - xs_important.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that gives the accuracy when using the *best $n$ features* for every possible value of $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_for_n_best_features(\n",
    "    feature_importance: pd.Series, xs: pd.DataFrame, y: pd.Series, valid_xs: pd.DataFrame, valid_y: pd.Series\n",
    ") -> pd.Series:\n",
    "    \n",
    "    n_features = len(feature_importance)\n",
    "    \n",
    "    acc = np.empty(n_features)\n",
    "    \n",
    "    for n_features_minus_1 in range(n_features):\n",
    "        \n",
    "        to_keep = feature_importance.iloc[:n_features_minus_1+1].index\n",
    "\n",
    "        # a model with only the relevant features\n",
    "        model = rf(xs[to_keep], y)\n",
    "        \n",
    "        acc[n_features_minus_1] = model_accuracy(model, valid_xs[to_keep], valid_y)\n",
    "    \n",
    "    return pd.Series(acc, index=range(1, 1+n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *aggregated* importance of the features is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "n_best_feactures_accuracy = accuracy_for_n_best_features(fi, xs, y, valid_xs, valid_y)\n",
    "n_best_feactures_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "n_best_feactures_accuracy[:50].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems beyond a few dozens of features, there is not much to be gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact on performance of the feature importance threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to assess the performance when dismissing those features whose performance is below a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_threshold_to_accuracy(\n",
    "    feature_importance: pd.Series, threshold: float, xs: pd.DataFrame, y: pd.Series, valid_xs: pd.DataFrame,\n",
    "    valid_y: pd.Series) -> float:\n",
    "    \n",
    "    # `True` for those features whose importance is above the treshold\n",
    "    feature_above_threshold = feature_importance >= threshold\n",
    "    \n",
    "    # their names are elicited\n",
    "    important_features = fi[feature_above_threshold].index\n",
    "    \n",
    "    # a new `DataFrame` for training with only the most important features\n",
    "    xs_important = xs[important_features]\n",
    "    \n",
    "    # a model with only the relevant features\n",
    "    model = rf(xs_important, y)\n",
    "    \n",
    "    # accuracy on the validation set\n",
    "    return model_accuracy(model, valid_xs[important_features], valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_importance = fi.iloc[0].item()\n",
    "max_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the best feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_threshold_to_accuracy(fi, max_importance, xs, y, valid_xs, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over a range of thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = np.linspace(max_importance / 1_000, max_importance, 10)\n",
    "thresholds = np.arange(1e-3, max_importance, 5e-3)\n",
    "threshold_accuracy = [importance_threshold_to_accuracy(fi, t, xs, y, valid_xs, valid_y) for t in thresholds]\n",
    "threshold_n_features = (fi.values[np.newaxis, :] > thresholds[:, np.newaxis]).sum(axis=1)\n",
    "threshold_accuracy_df = pd.DataFrame(\n",
    "    np.c_[thresholds, threshold_accuracy, threshold_n_features],\n",
    "    columns=['threshold', 'accuracy', 'n_features']\n",
    ").astype({'n_features': int})\n",
    "threshold_accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the *accuracy* against the *threshold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_accuracy_df.plot(x='threshold', y='accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a lower threshold (left part of the plot) entails more features being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability on features importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience function to estimate *feature importance* for a given split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_and_compute_feature_importance(i_testing_subjects: list, i_validation_subjects: list) -> pd.Series:\n",
    "\n",
    "    model, to = split_data_and_fit(df, i_testing_subjects, i_validation_subjects)\n",
    "\n",
    "    # a Pandas series with the importance of every feature\n",
    "    return feature_importance_series(model, to.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same data splitting considered so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_1 = split_data_and_compute_feature_importance(i_testing_subjects, i_validation_subjects)\n",
    "fi_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a different data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_2 = split_data_and_compute_feature_importance([0, 1], [2, 3])\n",
    "fi_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both results are assembled in a single `DataFrame` (properly pairing together values referred to the same feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.concat((fi_1, fi_2), axis=1)\n",
    "fi_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most *overall* important features when considering the two splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df.sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the *features importance* for every possible data split of the *training* set into *training* and *validation*, while excluding the *testing* subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis = [\n",
    "    split_data_and_compute_feature_importance(i_testing_subjects, list(i_valid))\n",
    "    for i_valid in itertools.combinations(range(12), 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_df = pd.concat(fis, axis=1)\n",
    "fis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the most *overall* important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_df.sum(axis=1).sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top features are not the same as before, but those that were important before (*GPDC* $116$ and $156$, *dDTF* $79$...) still are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the mean, we can also compute the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis_df.std(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviations are pretty small relative to the means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to split the data on individual subjects (as opposed to *paired* subjects) and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_on_individual_subjects_and_fit(\n",
    "    df: pd.DataFrame, i_healthy_subjects_in_valid: list, i_ill_subjects_in_valid: list):\n",
    "\n",
    "    # the names of the columns, which are NOT the dependent variable\n",
    "    data_columns = df.filter(regex=r'\\d+').columns.to_list()\n",
    "    \n",
    "    i_training, i_validation = split_data_on_individual_subjects(\n",
    "        df, i_healthy_subjects_in_valid, i_ill_subjects_in_valid)\n",
    "\n",
    "    # fastai data object\n",
    "    to = TabularPandas(df, cont_names=data_columns, y_names=dep_var, splits=(i_training, i_validation))\n",
    "\n",
    "    # the model is built and fitted\n",
    "    model = rf(to.train.xs, to.train.y)\n",
    "\n",
    "    return model, to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new function that encapsulates the application of the above function on every combination of *individual* subjects for `i_healthy_subjects_in_valid` and `i_ill_subjects_in_valid`. The names of the columns in the resulting `DataFrame` indicate the index of the healthy subject and that of the ill one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_individual_combinations():\n",
    "    \n",
    "    series = []\n",
    "    names = []\n",
    "\n",
    "    for i_healthy, i_ill in itertools.product(range(n_subjects), repeat=2):\n",
    "\n",
    "        # split and fitting\n",
    "        m, t = split_data_on_individual_subjects_and_fit(df, [i_healthy], [i_ill])\n",
    "\n",
    "        # results are recorded\n",
    "        series.append(feature_importance_series(m, t.xs))\n",
    "        names.append(f'{i_healthy}H{i_ill}I')\n",
    "\n",
    "    df_fi = pd.concat(series, axis=1)\n",
    "    df_fi.columns = names\n",
    "    \n",
    "    return df_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "df_fi = fi_individual_combinations()\n",
    "df_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "average_fi = df_fi.mean(axis=1).sort_values(ascending=False)\n",
    "average_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *sub*directory for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_aware_output_path = output_path / 'subject_aware'\n",
    "subject_aware_output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, a `DataFrame` focused only on subjects and whether or not they are ill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_df = df.groupby(['subject', 'ill']).first().reset_index()[['subject', 'ill']]\n",
    "subjects_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folder = StratifiedKFold(n_splits=n_folds_cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_metrics = []\n",
    "res_index = []\n",
    "res_sensitivity_specificity_auc = []\n",
    "res_feature_importance = []\n",
    "\n",
    "for _, i_valid in k_folder.split(subjects_df['subject'], subjects_df['ill']):\n",
    "    \n",
    "    # the subjects that will go into the validation set\n",
    "    valid_subjects = subjects_df.loc[i_valid.tolist()]['subject'].values\n",
    "    \n",
    "    # every subject is present twice (healthy and ill) in the validation set\n",
    "    assert (np.array(list(collections.Counter(valid_subjects).values())) == 2).all()\n",
    "    \n",
    "    # the valid subjects without duplicates\n",
    "    unique_valid_subjects = np.unique(valid_subjects)\n",
    "    \n",
    "    # the data is split and model fit on the training set\n",
    "    model, to = split_data_and_fit(df, [], unique_valid_subjects)\n",
    "    \n",
    "    # training and validation sets are extracted from tabular object `to`\n",
    "    xs, y = to.train.xs, to.train.y\n",
    "    valid_xs, valid_y = to.valid.xs, to.valid.y\n",
    "    \n",
    "    # accuracy on the training and validation sets\n",
    "    res_metrics.append((model_accuracy(model, xs, y), model_accuracy(model, valid_xs, valid_y)))\n",
    "    \n",
    "    # a label for the index\n",
    "    res_index.append(' & '.join([str(e) for e in unique_valid_subjects]))\n",
    "    \n",
    "    # sensitivity, specificity and AUC\n",
    "    res_sensitivity_specificity_auc.append(sensitivity_specificity_auc(model.predict_proba(valid_xs)[:, 1], valid_y))\n",
    "    \n",
    "    # feature importance\n",
    "    res_feature_importance.append(feature_importance_series(model, xs))\n",
    "\n",
    "AUCs = [e[2] for e in res_sensitivity_specificity_auc]\n",
    "metrics = pd.DataFrame(np.c_[res_metrics, AUCs], columns=['training', 'validation', 'AUC'])\n",
    "metrics.index = res_index\n",
    "metrics.index.name = 'sujects in validation'\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['validation'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average AUC is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['AUC'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenience function to write the (ordered) features importance into a *csv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_importance_to_csv(\n",
    "    features_importance: pd.Series, output_file: Union[str, pathlib.Path], n_rows: Optional[int] = 10) -> None:\n",
    "\n",
    "    with output_file.open('w') as f:\n",
    "        \n",
    "        f.write('# feature, importance\\n')\n",
    "\n",
    "    features_importance[:n_rows][::-1].to_csv(output_file, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class to output features' importance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesImportanceManager:\n",
    "    \n",
    "    def __init__(self, colors: set) -> None:\n",
    "        \n",
    "        self.colors = colors.copy()\n",
    "        self.feature_name_to_color = {}\n",
    "    \n",
    "    def summarize_and_write_importance_to_csv(\n",
    "        self, features_importance: list, output_path: Union[str, pathlib.Path], n_best: int = 10\n",
    "    ) -> Tuple[pd.DataFrame, set]:\n",
    "        \"\"\"\n",
    "        Processes a list of features importance `Series` (one per fold), and write the corresponding *csv* files.\n",
    "        \"\"\"\n",
    "\n",
    "        # in case a `str` was passed\n",
    "        output_path = pathlib.Path(output_path)\n",
    "\n",
    "        # in case the directory doesn't exist\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "\n",
    "        # a `DataFrame` with all the importance `Series`\n",
    "        features_importance_df = pd.concat(res_feature_importance, axis=1)\n",
    "\n",
    "        # a function to use along `replace` below\n",
    "        def repl(m):\n",
    "            return f'{m.group(2)}/' + \\\n",
    "                list(eeg_band_from_raw_freq)[int(m.group(1))] + \\\n",
    "                f' ch. {eeg_index_to_name[int(m.group(3))-1]} to {eeg_index_to_name[int(m.group(4))-1]}'\n",
    "\n",
    "        # labels in the index are renamed for the sake of *human* readability\n",
    "        features_importance_df.index = features_importance_df.index.str.replace(\n",
    "            r'f(\\d)_([a-zA-Z]+)_(\\d+)->(\\d+)', repl, regex=True)\n",
    "\n",
    "        # for every feature we find\n",
    "        features_importance_summary_df = pd.concat([\n",
    "            # ...the minimum importance value across all the folds, and\n",
    "            features_importance_df.min(axis=1).rename('Minimum'),\n",
    "            # ... also the average\n",
    "            features_importance_df.mean(axis=1).rename('Average')\n",
    "        ], axis=1)\n",
    "        \n",
    "        #\n",
    "        self.features_importance_summary_df = features_importance_summary_df\n",
    "\n",
    "        # the `n_best` features in every case\n",
    "        best_min_features_importance = features_importance_summary_df['Minimum'].sort_values(ascending=False)[:n_best]\n",
    "        best_average_features_importance = features_importance_summary_df['Average'].sort_values(\n",
    "            ascending=False)[:n_best]\n",
    "\n",
    "        # features that are in both rankings\n",
    "        common_features = set(best_min_features_importance.index).intersection(\n",
    "            set(best_average_features_importance.index))\n",
    "        \n",
    "        new_colors = common_features - set(self.feature_name_to_color.keys())\n",
    "\n",
    "        # there are enough different colors\n",
    "        assert len(new_colors) <= len(self.colors)\n",
    "\n",
    "        # a mapping from feature to color\n",
    "        mapping_update = dict(zip(sorted(list(new_colors)), sorted(list(self.colors))))\n",
    "        \n",
    "        self.colors -= set(mapping_update.values())\n",
    "        \n",
    "        self.feature_name_to_color.update(mapping_update)\n",
    "\n",
    "        # a function that adds color to any label in the `Series`' index that is in the above mapping\n",
    "        def color_features_names(fi: pd.Series) -> pd.Series:\n",
    "\n",
    "            return fi.set_axis(\n",
    "                [f'\\\\textcolor{{{self.feature_name_to_color[n]}}}{{\\\\textbf{{{n}}}}}'\n",
    "                 if n in self.feature_name_to_color\n",
    "                 else f'\\\\textcolor{{gray}}{{{n}}}' for n in fi.index])\n",
    "\n",
    "        write_importance_to_csv(\n",
    "            color_features_names(best_min_features_importance), output_path / 'minimum_importance.txt', n_rows=None)\n",
    "        write_importance_to_csv(\n",
    "            color_features_names(best_average_features_importance),\n",
    "            output_path / 'average_importance.txt', n_rows=None)\n",
    "\n",
    "        return features_importance_df, common_features\n",
    "\n",
    "features_importance_manager = FeaturesImportanceManager(latex_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*csv*'s with features importance are written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_aware_feature_importance_df, subject_aware_common_features = features_importance_manager.summarize_and_write_importance_to_csv(res_feature_importance, subject_aware_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *average* across folds for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_aware_feature_importance_mean = subject_aware_feature_importance_df.mean(axis=1)\n",
    "subject_aware_feature_importance_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *bar plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_aware_feature_importance_mean.plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_aware_common_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features account for this percentage of *importance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_aware_feature_importance_mean.loc[subject_aware_common_features].sum() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while making up this percentage out of the overall number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subject_aware_common_features) / subject_aware_feature_importance_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_manager.features_importance_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_manager.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to save the ROC curves into *csv* files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rocs_to_csvs(sensitivity_specificity_auc: list, output_path: Union[str, pathlib.Path]) -> None:\n",
    "    \n",
    "    for i_fold, (specificity, sensitivity, _) in enumerate(sensitivity_specificity_auc):\n",
    "\n",
    "        output_file = output_path / f'fold_{i_fold}.txt'\n",
    "\n",
    "        # header\n",
    "        with output_file.open('w') as f:\n",
    "            f.write('# 1-specificity, sensitivity\\n')\n",
    "        \n",
    "        # data\n",
    "        with output_file.open('a') as f:\n",
    "            np.savetxt(f, np.stack([1. - specificity, sensitivity]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is used on the above results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rocs_to_csvs(res_sensitivity_specificity_auc, subject_aware_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenience function, this one to write the AUCs to a (single) *csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_auc_to_csv(auc: np.array, output_path: Union[str, pathlib.Path]) -> None:\n",
    "\n",
    "    output_file = output_path / 'AUC.txt'\n",
    "\n",
    "    # header\n",
    "    with output_file.open('w') as f:\n",
    "        f.write('# AUC\\n')\n",
    "    \n",
    "    # data\n",
    "    with output_file.open('a') as f:\n",
    "        np.savetxt(f, auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_auc_to_csv(metrics['AUC'].values, subject_aware_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without splitting samples by subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *sub*directory for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_unaware_output_path = output_path / 'subject_unaware'\n",
    "subject_unaware_output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An object that will produce an iterator to go through the different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = StratifiedKFold(n_splits=n_folds_cross_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first subject (for instance) is used as a model for exacting training and testing *indexes* *all* the subjects. This is accomplished using the `nth` method of a *grouped* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_subject_df = df[df['subject']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is grouped by *subject*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_subject = df.groupby('subject', as_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and validate for every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_metrics = []\n",
    "res_indexes = []\n",
    "res_sensitivity_specificity_auc = []\n",
    "res_feature_importance = []\n",
    "\n",
    "\n",
    "for i_train, i_valid in folder.split(first_subject_df.index, first_subject_df['ill']):\n",
    "    \n",
    "    splits = (grouped_by_subject.nth(i_train.tolist()).index.tolist(),\n",
    "              grouped_by_subject.nth(i_valid.tolist()).index.tolist())\n",
    "    \n",
    "    to = TabularPandas(df, cont_names=data_columns, y_names=dep_var, splits=splits)\n",
    "    \n",
    "    xs, y = to.train.xs, to.train.y\n",
    "    valid_xs, valid_y = to.valid.xs, to.valid.y\n",
    "    \n",
    "    model = rf(xs, y)\n",
    "    \n",
    "    # feature importance\n",
    "    res_feature_importance.append(feature_importance_series(model, xs))\n",
    "    \n",
    "    res_metrics.append([model_accuracy(model, xs, y), model_accuracy(model, valid_xs, valid_y)])\n",
    "    res_indexes.append(splits)\n",
    "    \n",
    "    res_sensitivity_specificity_auc.append(sensitivity_specificity_auc(model.predict_proba(valid_xs)[:, 1], valid_y))\n",
    "\n",
    "auc = [r[2] for r in res_sensitivity_specificity_auc]\n",
    "folded_accuracy_df = pd.DataFrame(np.c_[res_metrics, auc], columns=['training', 'validation', 'AUC'])\n",
    "del res_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean in the validation set is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folded_accuracy_df['validation'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*csv*'s with features importance are written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_unaware_feature_importance, subject_unaware_common_features = features_importance_manager.summarize_and_write_importance_to_csv(\n",
    "    res_feature_importance, subject_unaware_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC values are written to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_unaware_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_auc_to_csv(folded_accuracy_df['AUC'].values, subject_unaware_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Receiver Operating Characteristic* (ROC) for every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for sensitivity, specificy, _ in res_sensitivity_specificity_auc:\n",
    "    ax.plot(1.-specificy, sensitivity)\n",
    "ax.set_xlim((-0.01, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for every fold are saved in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rocs_to_csvs(res_sensitivity_specificity_auc, subject_unaware_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check: every index made into the validation set exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the indexes in the validation sets in all the folds\n",
    "i_validation_aggregated = sum([e[1] for e in res_indexes], [])\n",
    "\n",
    "# every element in the above list is different, and the number of elements is exactly the overall number of samples\n",
    "assert len(i_validation_aggregated) == len(np.unique(i_validation_aggregated)) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizes of the training and validation sets in every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_df = pd.DataFrame([[len(i_t), len(i_v)] for i_t, i_v in res_indexes], columns=['training', 'validation'])\n",
    "folds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples per subject and that of folds for cross-validation may yield an odd number of samples (from every subject) in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_subject = grouped_by_subject.size()['size'].iloc[0]\n",
    "n_samples_per_subject / n_folds_cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that's the case, we will not have the same number of healthy and ill examples (for every subject) nor in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_subject.nth(i_train.tolist()).groupby(['subject', 'ill']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nor in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_subject.nth(i_valid.tolist()).groupby(['subject', 'ill']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(numbers above refer to the last fold since we are using the last state of the above loop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aware vs. Unaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "always_relevant_features = subject_aware_common_features.intersection(subject_unaware_common_features)\n",
    "always_relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Left-over* colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_manager.colors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
