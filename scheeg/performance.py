# AUTOGENERATED! DO NOT EDIT! File to edit: 20_performance.ipynb (unless otherwise specified).

__all__ = ['accuracy', 'model_accuracy', 'sensitiv_specificity_from_learner', 'sensitivity_specificity_auc']

# Cell
from typing import Union, Tuple

import numpy as np
import pandas as pd
import sklearn.metrics
import matplotlib.pylab as plt

from fastai.interpret import ClassificationInterpretation

# Cell
def accuracy(preds: np.array, y: pd.Series) -> float:

    # bad things happen if a `DataFrame` is passed
    assert isinstance(y, pd.Series)

#     return (y.astype(bool).to_numpy() == (preds > 0.5) ).mean()
    return (y.values == preds ).mean()

# Cell
def model_accuracy(model, xs, y) -> float:
    return accuracy(model.predict(xs), y)

# Cell
def sensitiv_specificity_from_learner(learner):

    interp = ClassificationInterpretation.from_learner(learner)

    upp, low = interp.confusion_matrix()
    tn, fp = upp[0], upp[1]
    fn, tp = low[0], low[1]

    sensitivity = tp/(tp + fn)
    specificity = tn/(fp + tn)

    return sensitivity, specificity

# Cell
def sensitivity_specificity_auc(
    preds, y, return_thresholds: bool = False
) -> Union[Tuple[np.array, np.array, float], Tuple[np.array, np.array, float, np.array]]:

    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y, preds)
    auc = sklearn.metrics.auc(fpr, tpr)

    # result encompasses at least sensitivity, specificity, and AUC
    res = tpr, 1.-fpr, auc

    if return_thresholds:

        return res + (thresholds,)

    else:

        return res